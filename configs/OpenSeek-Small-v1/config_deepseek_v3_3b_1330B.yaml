# DeepSeek 16B, 2.4A Model
defaults:
  - _self_
  - train: train_deepseek_v3_3b_1330B.yaml

experiment:
  exp_name: Aquila-1_4B-A0_4B-1330B
  dataset_base_dir: your/dataset/path
  seed: 42
  save_steps: 600
  load: null
  exp_dir: ${experiment.exp_name}
  #ckpt_format: torch_dist
  ckpt_format: torch
  task:
    type: train
    backend: megatron
    entrypoint: flagscale/train/train_gpt.py
  runner:
    no_shared_fs: false
    backend: torchrun
    rdzv_backend: static
    ssh_port: 22
    nnodes: 8
    nproc_per_node: 8
  cmds:
    before_start: "ulimit -n 1048576 && source /root/miniconda3/bin/activate flagscale"
  envs:
    VISIBLE_DEVICES: 0,1,2,3,4,5,6,7
    DEVICE_MAX_CONNECTIONS: 1

action: run

hydra:
  run:
    dir: ${experiment.exp_dir}/hydra
