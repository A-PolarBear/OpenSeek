defaults:
  - train: train_aquila_3b 
  - _self_

experiment:
  exp_name: aquila_3b_exp02
  exp_dir: ${experiment.exp_name}
  task:
    type: train
    backend: megatron
    entrypoint: ./flagscale/train/train_aquila.py
  runner:
    backend: torchrun
    rdzv_backend: static
    ssh_port: 22
    nnodes: 8
    nproc_per_node: 16 
    hostfile: ./hostfile
  envs:
    CUDA_VISIBLE_DEVICES: 0,1,2,3,4,5,6,7
    CUDA_DEVICE_MAX_CONNECTIONS: 1
    CCL_IB_DISABLE: 0
    NCCL_IB_CUDA_SUPPORT: 1
    NCCL_IB_GID_INDEX: 0
    #TORCH_NCCL_USE_COMM_NONBLOCKING: 1
    OMP_NUM_THREADS: 4
    ENABLE_FLASH_ATTENTION_WITH_IXDNN: 0
    NCCL_NET_PLUGIN: none
    #NCCL_SHM_DISABLE=1
    NCCL_ALGO: Ring
    NCCL_P2P_NET_CHUNKSIZE: 1048576
    NCCL_CHUNK_SIZE: 1048576
    NCCL_BUFFSIZE: 8388608
    NCCL_MAX_NCHANNELS: 4
    NCCL_MIN_NCHANNELS: 4
    NCCL_MAX_P2P_NCHANNELS: 1
    NCCL_PROTO: Simple
    NCCL_NET_SHARED_BUFFERS: 0
    NCCL_P2P_LL_THRESHOLD: 0
    IXCCL_MIX_NV: 1
    IXCCL_FUSED_ENABLE: 0
    NCCL_IB_DISABLE: 0
    NCCL_IB_HCA: mlx5_2,mlx5_8action: run

hydra:
  run:
    dir: ${experiment.exp_dir}/hydra 
